{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "sns_red = '#e77c8d'\n",
    "sns_blue = '#5ea5c5'\n",
    "sns_green = '#56ad74'\n",
    "sns_purple = '#a291e1'\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "DATA_PATH = './../data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Opening the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'^pc.*\\.xls$', re.IGNORECASE)\n",
    "files = [f for f in glob('*', root_dir=DATA_PATH) if pattern.match(f)]\n",
    "\n",
    "if files:\n",
    "    print('Found Files:')\n",
    "    for i, file in enumerate(files):\n",
    "        print(f'{i+1} - \"{file}\"')\n",
    "\n",
    "    # this is used when saving files\n",
    "    common_name = re.findall(r'pc.*\\(\\d{1,3}mgml\\)', files[0], re.IGNORECASE)[0].replace(' ', '_').lower()\n",
    "    print(f\"\\nFiles related to -> {common_name}\")\n",
    "    if not common_name:\n",
    "        common_name = ''\n",
    "\n",
    "else:\n",
    "    raise Exception('No files were found !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interested_in = ['Storage modulus', 'Loss modulus', 'Tan(delta)', 'Temperature']\n",
    "\n",
    "dfs = [pd.read_excel(f\"{DATA_PATH}/{file}\",\n",
    "                     index_col=None,\n",
    "                     header=1,\n",
    "                     sheet_name=1,\n",
    "                     usecols=interested_in,\n",
    "                     na_values=(\"NA\", \"N/A\", \"na\", \"n/a\", \"NULL\", \"null\", \"Not documented\", \"Not Documented\", 'nan', '')).drop(labels=0, axis=0).reset_index(drop=True)\n",
    "       for file in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Another way (more robust) to get the same result, but it takes longer to run\n",
    "```python\n",
    "interested_in = ['Storage modulus', 'Loss modulus', 'Tan(delta)', 'Temperature']\n",
    "dfs = []\n",
    "\n",
    "for file in files:\n",
    "    t = pd.read_excel(file,\n",
    "                      index_col=None,\n",
    "                      header=[1,2],\n",
    "                      sheet_name=1,\n",
    "                      na_values=(\"NA\", \"N/A\", \"na\", \"n/a\", \"NULL\", \"null\", \"Not documented\", \"Not Documented\", 'nan', ''))\n",
    "    t.columns = t.columns.get_level_values(0)\n",
    "    t = t.loc[:, interested_in]\n",
    "    dfs.append(t)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Calculating the average values\n",
    "Just averaging out the data will account for the random uncertainty in the measurements, but not the systematic uncertainty. To account for the systematic uncertainty, we need to calculate the standard deviation of the data and use it to calculate the error bars.\n",
    "\n",
    "The uncertainty in the average value is given by the standard deviation of the data divided by the square root of the number of measurements.\n",
    "(video -> (Max - Min) / 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use temp bins to create the average values rather than just going row by row, maybe you can use pd.cut or pd.categorical see below\n",
    "# minv = int(np.min(temp))\n",
    "# maxv = ceil(np.max(temp)) + 10\n",
    "# bins = list(range(minv, maxv, round((maxv - minv) / 5))) + [maxv + 1]\n",
    "# temp_df['new_col'] = pd.cut(df.loc[:, col].copy(), bins)\n",
    "\n",
    "min_length = min({df.shape[0] for df in dfs})\n",
    "max_length = max({df.shape[0] for df in dfs})\n",
    "\n",
    "# checking the Excel files length is acceptable\n",
    "# change the \"5\" to any number you think is acceptable, or change the 'ignore_allowable_diff' to True if you want to ignore this part\n",
    "allowable_diff = 5\n",
    "ignore_allowable_diff = False\n",
    "\n",
    "if max_length - min_length > allowable_diff and not ignore_allowable_diff:\n",
    "    message = ''\n",
    "    for i, file in enumerate(files):\n",
    "        message = message + f'{file} -> {len(dfs[i])} rows\\n'\n",
    "    raise Exception(f\"The Excel files are more than {allowable_diff} rows different in length \\n{message}\")\n",
    "\n",
    "av_df = pd.DataFrame(columns=interested_in)\n",
    "num_of_files = len(dfs)\n",
    "\n",
    "# TODO: It is not recommended to build DataFrames by adding single rows in a for loop. Build a list of rows and make a DataFrame in a single concat.\n",
    "\n",
    "for i in range(min_length):\n",
    "    for col in interested_in:\n",
    "        av_df.loc[i, col] = round(sum([df.loc[i, col] for df in dfs]) / num_of_files, 5)\n",
    "\n",
    "av_df.head(3)\n",
    "\n",
    "# Exporting the averaged data\n",
    "av_df.to_excel(f'{DATA_PATH}/av_data_{common_name}.xlsx', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = av_df.copy()\n",
    "x = new_df.loc[:, 'Temperature'].to_numpy().astype(np.float16)\n",
    "y1 = new_df.loc[:, 'Storage modulus'].to_numpy().astype(np.float16)\n",
    "y2 = new_df.loc[:, 'Loss modulus'].to_numpy().astype(np.float16)\n",
    "y3 = new_df.loc[:, 'Tan(delta)'].to_numpy().astype(np.float16)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for df in dfs:\n",
    "    sns.scatterplot(data=df, x='Temperature', y=\"Storage modulus\", ax=ax, s=10, color=sns_red)\n",
    "    sns.scatterplot(data=df, x='Temperature', y=\"Loss modulus\", ax=ax, s=10, color=sns_blue)\n",
    "    sns.scatterplot(data=df, x='Temperature', y=\"Tan(delta)\", ax=ax, s=10, color=sns_green)\n",
    "\n",
    "ax.set_ylabel('Modulus (GPa)')\n",
    "ax.legend(['Storage modulus', 'Loss modulus', 'Tan(delta)'])\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(f'{DATA_PATH}/all_data_{common_name}.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.1. Looking At The 95% Confidence Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = interested_in.copy()\n",
    "temp.append('source')\n",
    "err_df = pd.DataFrame(columns=interested_in)\n",
    "av_temperature = np.zeros(min_length)\n",
    "\n",
    "# Getting the average temperatures\n",
    "for df in dfs:\n",
    "    df = df.iloc[:min_length, :].copy()\n",
    "    av_temperature = av_temperature + df.loc[:, 'Temperature'].to_numpy().astype(np.float16)\n",
    "av_temperature = (av_temperature / len(dfs)).round(5)\n",
    "\n",
    "# concatenating the data\n",
    "for i, df in enumerate(dfs):\n",
    "    df = df.iloc[:min_length, :].copy()\n",
    "    df.loc[:, 'Temperature'] = av_temperature\n",
    "    df.loc[:, 'source'] = np.zeros(min_length) + i\n",
    "    err_df = pd.concat([err_df, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# TODO: since we have only 3-4 replications, is it statistically significance to use confidence interval? `errorbar=(\"ci\", 95)`\n",
    "# TODO: if we can use ci, should I do Grand-mean centering?\n",
    "\n",
    "sns.lineplot(data=err_df, x='Temperature', y=\"Storage modulus\", ax=ax, errorbar=\"se\", color=sns_red)\n",
    "sns.lineplot(data=err_df, x='Temperature', y=\"Loss modulus\", ax=ax, errorbar=\"se\", color=sns_blue)\n",
    "sns.lineplot(data=err_df, x='Temperature', y=\"Tan(delta)\", ax=ax, errorbar=\"se\", color=sns_green)\n",
    "\n",
    "ax.set_ylabel('Modulus (GPa)')\n",
    "ax.legend(['Storage modulus', '', 'Loss modulus', '', 'Tan(delta)', ''])\n",
    "fig.tight_layout()\n",
    "\n",
    "# 95%_confidence_interval\n",
    "fig.savefig(f'{DATA_PATH}/standard error_{common_name}.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.2. Smoothing the data and calculating the gradient\n",
    "Applying a simple moving average to smooth the data\n",
    "\n",
    "### 4.2.1. Explanation of the code\n",
    "- `w = 25` sets the width of the moving average window to 25 data points.\n",
    "- `smooth_y = np.convolve(y1, np.ones(w), 'valid') / w` calculates the moving average of the y1 data by convolving it with a window of ones of width w, and then dividing the result by `w`. The **\"valid\"** argument ensures that the output has the same length as the input.\n",
    "- `smooth_x = x[w//2 : -w//2+1]` creates a new array containing the `x` values that correspond to the smoothed data. The slicing operation here removes the first and last `w//2` values of `x` to match the length of `smooth_y`.\n",
    "\n",
    "The `convolve()` function performs a linear convolution, which is a mathematical operation that computes the integral of the product of two functions as one of them is reversed and shifted over the other. **In the context of signal processing, convolution is a way to combine two signals, and is often used for tasks such as filtering and smoothing.**\n",
    "\n",
    "The `w // 2` is the **integer division** of w by 2, which gives the number of points to remove from the beginning and end of the x array to match the length of the smoothed data.\n",
    "For example, if w is 25, `w // 2` is 12. Then, `-w // 2` is -13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the window to anything less than 3 will not smooth the data\n",
    "w = len(x) // 3\n",
    "if w > 2:\n",
    "    smooth_y = np.convolve(y1, np.ones(w), 'valid') / w\n",
    "    smooth_x = x[(w // 2): (-w // 2) + 1]\n",
    "else:\n",
    "    smooth_y = y1\n",
    "    smooth_x = x\n",
    "\n",
    "# We need a dataframe to use the query function and filter out the data. Otherwise, we would have to use masking and boolean indexing.\n",
    "grad = pd.DataFrame([smooth_x, smooth_y]).transpose()\n",
    "grad.columns = ['temperature', 'gradient']\n",
    "threshold = 0.5\n",
    "start_point = min(grad.query(f'gradient > {threshold}')['temperature'])\n",
    "# end_point = max(grad.query(f'gradient > {threshold}')['temperature'])     # TODO: find a way to find the end point\n",
    "end_point = max(x)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=125)\n",
    "sns.lineplot(x=grad['temperature'],\n",
    "             y=grad['gradient'],\n",
    "             ax=ax,\n",
    "             color=sns_blue)\n",
    "ax.axvline(x=start_point, color=sns_purple, linestyle='--')\n",
    "# ax.axvline(x=end_point, color='red', linestyle='--')\n",
    "\n",
    "print(f\"start point: {start_point}\")\n",
    "print(f\"end point  : {end_point}\")\n",
    "\n",
    "fig.savefig(f'{DATA_PATH}/gradient_{common_name}.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.3. Plotting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the colors\n",
    "plt.style.use('ggplot')\n",
    "# change the size\n",
    "# plt.style.use('seaborn-poster')\n",
    "plt.style.use('seaborn-paper')\n",
    "# change background\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "st = ax.scatter(x, y1, color=sns_red, marker='o', s=10, alpha=0.7);\n",
    "ls = ax.scatter(x, y2, color=sns_blue, marker='o', facecolor='none', s=10)\n",
    "tan = ax.scatter(x, y3, color=sns_green, marker='^', s=10, alpha=0.7)\n",
    "\n",
    "ax.legend(['Storage modulus', 'Loss modulus', 'Tan(delta)'])\n",
    "\n",
    "#ax.set_xlim(10, 85);\n",
    "#ax.set_ylim(0.01 , 1000);\n",
    "\n",
    "#ax.axvline(x=start_point, color=sns_purple, linestyle='--')\n",
    "#ax.axvline(x=end_point, color=sns_purple, linestyle='--')\n",
    "\n",
    "fig.savefig(f'{DATA_PATH}/av_values_{common_name}.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Curve fitting Storage M.\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{L}{1 + exp(-k * (x - x_0))} + b\n",
    "$$\n",
    "\n",
    "- **L** is responsible for scaling the output range from [0,1] to [0,L]\n",
    "- **b** adds bias to the output and changes its range from [0,L] to [b,L+b]\n",
    "- **k** is responsible for scaling the input, which remains in (-inf,inf)\n",
    "- **x0** is the point in the middle of the Sigmoid, i.e. the point where Sigmoid should originally output the value 1/2\n",
    "if $x=x_0$, we get $$\\frac{1}{1+exp(0)} = 0.5$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x, L ,x0, k, b):\n",
    "    y = L / (1 + np.exp(-k * (x - x0))) + b\n",
    "    return (y)\n",
    "\n",
    "filtered_df = av_df[(av_df['Temperature'] > start_point)].copy()\n",
    "x_filtered = filtered_df.loc[:, 'Temperature'].to_numpy().astype(np.float16)\n",
    "y1_filtered = filtered_df.loc[:, 'Storage modulus'].to_numpy().astype(np.float16)\n",
    "\n",
    "y1_log = np.log10(y1_filtered)\n",
    "\n",
    "p0 = [max(y1_filtered), np.median(x_filtered), 1, min(y1_filtered)]\n",
    "\n",
    "popt, pcov = curve_fit(sigmoid, x_filtered, y1_log, p0, method='lm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1. Looking at the fitted curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the colors\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn-paper')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "L_opt ,x0_opt, k_opt, b_opt = popt\n",
    "\n",
    "x_model = np.linspace(min(x_filtered), max(x_filtered), 100)\n",
    "y_model = sigmoid(x_model, L_opt, x0_opt, k_opt, b_opt)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "\n",
    "ax.scatter(x_filtered, y1_log, color=sns_blue, marker='o', s=20)\n",
    "ax.plot(x_model, y_model, color=sns_red)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.2. Looking at covariance matrix\n",
    "The diagonals provide the variance of the parameter estimate.\n",
    "One standard deviation errors on the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perr = np.sqrt(np.diag(pcov))\n",
    "labels = ['L', 'x0', 'k', 'b']\n",
    "sns.barplot(x=labels, y=perr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The result is 4x4 heat map. The main diagonal is related to the error (from top left to bottom right) for L, x0, k, b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=[6, 6], dpi=100)\n",
    "\n",
    "ax.grid(which='minor', color='black', linestyle='-', linewidth=1, alpha=1)\n",
    "ax.grid(which='major', alpha=0)\n",
    "\n",
    "im = ax.imshow(error_data:=np.log(np.abs(pcov)), cmap=sns.cubehelix_palette(as_cmap=True, reverse=True))\n",
    "# plt.cm.inferno)\n",
    "\n",
    "# Major ticks\n",
    "ax.set_yticks(np.arange(0, 4, 1))\n",
    "ax.set_xticks(np.arange(0, 4, 1))\n",
    "\n",
    "# Labels for major ticks\n",
    "ax.set_yticklabels(labels, fontsize=12)\n",
    "ax.set_xticklabels(labels, fontsize=12)\n",
    "\n",
    "# setting minor ticks for grid\n",
    "ax.set_xticks(np.arange(-.5, 3.5, 1), minor=True, alpha=0)\n",
    "ax.set_yticks(np.arange(-.5, 3.5, 1), minor=True, alpha=0)\n",
    "\n",
    "# Creating a more advanced color map\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", \"4%\", pad=\"2%\")\n",
    "cbar = plt.colorbar(im, cax=cax)\n",
    "cbar.ax.set_ylabel(ylabel=\"Error\", rotation=90, fontsize=12)\n",
    "cbar.ax.set_yticklabels(labels=np.arange(round(error_data.min(), 1), round(error_data.max(), 1), 0.5), fontsize=12)\n",
    "cbar.minorticks_on()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
